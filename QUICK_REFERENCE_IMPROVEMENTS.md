# Quick Reference: Entity Name Resolution Improvements

## The Problem in One Chart

```
Current Performance:
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ F1: 82.4%  â”‚  Precision: 93%  â”‚  Recall: 74%  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

False Negatives: 399 pairs (26% of matches missed)

Where are they?
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Category           â”‚ Count â”‚  % â”‚ What they look like â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ OTHER (threshold)  â”‚  193  â”‚ 48%â”‚ High scores, rejectedâ”‚
â”‚ ABBREVIATION       â”‚  144  â”‚ 36%â”‚ IBM vs Int'l Bus...  â”‚
â”‚ NICKNAME           â”‚   44  â”‚ 11%â”‚ AmEx vs American...  â”‚
â”‚ TRANSLITERATION    â”‚   40  â”‚ 10%â”‚ NestlÃ© vs Nestle     â”‚
â”‚ PARTIAL_NAME       â”‚   25  â”‚  6%â”‚ Apple vs Apple Inc   â”‚
â”‚ Others             â”‚   18  â”‚  5%â”‚ Various patterns     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

KEY INSIGHT: 37.6% (150 pairs) scored 80-100% but were REJECTED!
```

---

## The Root Cause

**GMM Threshold Too Conservative:**

```
Current Setup:
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
  0%    44.7%   55.5% 58.2%            87.8%     100%
  â”œâ”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”¤
 Reject  T_LOW   S_90 T_HIGH        GMM Mean  Perfect
                      â†‘
                  TOO LOW!
                  (requires 98% certainty)

150 pairs scored 80-100% but fell below T_HIGH (58.2%)
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

False Negative Distribution:
         â”Œâ”€â”€â”€â”€â”€â”€ Should be accepted! â”€â”€â”€â”€â”€â”€â”
  0-20%: â”‚                                  â”‚  (0 pairs)
 20-40%: â–ˆâ–ˆâ–ˆ                                   (9 pairs)
 40-60%: â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                          (78 pairs)
 60-80%: â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ              (156 pairs) â† Gray zone
80-100%: â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                  (150 pairs) â† CRITICAL!
         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## Solution #1: Adjust Threshold (HIGHEST PRIORITY)

**One line change = +8-12% recall**

```python
# Before:
self.t_high_threshold = self.find_threshold_for_posterior(0.98)  # Too conservative

# After:
self.t_high_threshold = self.find_threshold_for_posterior(0.90)  # More balanced

# File: backend/app/services/gmm_threshold_service.py
# Function: calculate_adaptive_thresholds()
# Line: ~150
```

**Impact:**
- âœ… Captures 150 high-confidence pairs (80-100%)
- âœ… Recall: 74% â†’ 82-86% (+8-12%)
- âš ï¸ Precision: 93% â†’ 89-91% (-2-4%)
- âœ… F1: 82.4% â†’ 85-88% (+3-6%)
- â±ï¸ Implementation: 2-4 hours

---

## Solution #2: Add Acronym Detection (HIGH VALUE)

**36% of false negatives are abbreviations**

**Examples:**
- "JPM" â†” "JPMorgan Chase & Co."
- "IBM" â†” "International Business Machines"
- "MSFT" â†” "Microsoft Corporation"
- "AmEx" â†” "American Express Company"

**Code to add:**

```python
def calculate_abbreviation_score(self, short_name: str, long_name: str) -> float:
    """Returns 0.0-1.0 score for abbreviation relationship."""
    long_words = long_name.split()
    initials = ''.join([w[0] for w in long_words])

    # Exact acronym match (JPM = JPMorgan Chase)
    if short_name.lower() == initials.lower():
        return 1.0

    # Fuzzy acronym (skip some words)
    if len(short_name) <= 5 and short_name.isupper():
        # Try subsequences: JPM from J[Morgan]C[hase]
        from itertools import combinations
        for combo in combinations(range(len(long_words)), len(short_name)):
            candidate = ''.join([long_words[i][0] for i in combo])
            if short_name.lower() == candidate.lower():
                return 0.9

    # Partial containment
    if short_name.lower() in long_name.lower():
        return min(0.8, len(short_name) / len(long_name) * 2)

    return 0.0

# In calculate_confidence():
abbrev_score = 0.0
if len(norm1) < len(norm2) * 0.5:
    abbrev_score = self.calculate_abbreviation_score(norm1, norm2)
elif len(norm2) < len(norm1) * 0.5:
    abbrev_score = self.calculate_abbreviation_score(norm2, norm1)

if abbrev_score > 0.7:
    base_score += (abbrev_score * 15)  # +15 point boost
```

**Impact:**
- âœ… Fixes 144 abbreviation pairs (36% of FNs)
- âœ… Recall: 74% â†’ 79-82% (+5-8%)
- âœ… Precision: 93% â†’ 93-94% (no loss!)
- âœ… F1: 82.4% â†’ 85-87% (+3-5%)
- â±ï¸ Implementation: 1-2 days

---

## Solution #3: Hybrid Threshold (Optional)

**Only if #1 + #2 don't reach target**

```python
def group_similar_names_hybrid(self, names):
    # Pass 1: Fixed 85% threshold (high confidence)
    # Pass 2: GMM adaptive (borderline cases)

    # Captures both high-confidence matches and
    # data-driven borderline decisions
```

**Impact:**
- âœ… Recall: 74% â†’ 77-79% (+3-5%)
- âš ï¸ Precision: 93% â†’ 91-92% (-1-2%)
- âœ… F1: 82.4% â†’ 83-85% (+1-3%)
- â±ï¸ Implementation: 4-8 hours

---

## Combined Impact

```
Current:     F1: 82.4%  (Recall: 74%, Precision: 93%)

After #1:    F1: 85-88% (Recall: 82-86%, Precision: 89-91%)
             â†‘ +3-6% F1

After #1+#2: F1: 86-90% (Recall: 84-90%, Precision: 89-91%)
             â†‘ +4-8% F1
             âœ… PRODUCTION READY

Target:      F1: 90%+ (industry leading)
```

---

## Implementation Priority

```
Week 1: ğŸ¯ Recommendation #1 (Threshold Adjustment)
â”œâ”€ Change P(same|score) from 0.98 to 0.90
â”œâ”€ Test on validation set
â”œâ”€ Tune for optimal F1
â””â”€ Expected: +8-12% recall (74% â†’ 82-86%)

Week 2-3: ğŸ¯ Recommendation #2 (Acronym Detection)
â”œâ”€ Add calculate_abbreviation_score()
â”œâ”€ Integrate with confidence scoring
â”œâ”€ Test on known acronyms
â””â”€ Expected: +5-8% recall (stacked)

Week 4: ğŸ“Š Evaluate & Fine-tune
â”œâ”€ If F1 < 88%, consider Recommendation #3
â”œâ”€ Otherwise, optimize #1 and #2 parameters
â””â”€ Final target: F1 86-90%
```

---

## Why This Will Work

**Evidence from the data:**

1. **High scores are being rejected**
   - 150 pairs scored 80-100% â†’ rejected by threshold
   - Mean FN confidence: 68.7% (very close to T_HIGH 58.2%)
   - This is a threshold calibration issue, not a scoring issue

2. **Individual matchers are working well**
   - Mean WRatio: 76.1/100 (fuzzy matching good)
   - Mean Token Set: 63.8/100 (token overlap good)
   - Mean Embedding: 71.7/100 (semantic similarity good)
   - All components performing; threshold is bottleneck

3. **Abbreviations are a clear pattern**
   - 36% of FNs are abbreviation pairs
   - Acronym detection is high-precision (few FPs)
   - Direct solution to a well-defined problem

4. **Current precision is high (93%)**
   - Room to trade 2-4% precision for 10-16% recall
   - Final precision 89-91% is still excellent
   - F1 improvement is worth the trade-off

---

## Risk Mitigation

**Test before deploying:**
- [ ] Create 20% validation holdout
- [ ] Implement on validation set first
- [ ] Verify precision > 90%
- [ ] A/B test against current system
- [ ] Document rollback procedure

**Monitoring:**
- Track false positive rate (keep < 10%)
- Check processing time (keep increase < 10%)
- Review new FP examples manually
- Validate on edge cases (acronyms, typos, etc.)

---

## Quick Decision Matrix

| Recommendation | Effort | Impact | Risk | Priority |
|----------------|--------|--------|------|----------|
| #1: Threshold  | 2-4 hrs | +8-12% | LOW  | â­â­â­ |
| #2: Acronyms   | 1-2 days| +5-8%  | LOW  | â­â­   |
| #3: Hybrid     | 4-8 hrs | +3-5%  | MED  | â­     |

**Recommended:** Do #1 first (quick win), then #2 (targeted fix).

---

## Files to Modify

### Recommendation #1:
- `backend/app/services/gmm_threshold_service.py` (line ~150)
  - Change `self.find_threshold_for_posterior(0.98)` to `(0.90)`

### Recommendation #2:
- `backend/app/services/name_matcher.py` (line ~200-250)
  - Add `calculate_abbreviation_score()` method
  - Modify `calculate_confidence()` to use it

### Configuration:
- `backend/app/config/settings.py`
  - Add `ABBREVIATION_BOOST_WEIGHT = 15.0`
  - Update GMM threshold comments

---

## Success Criteria

**Target Metrics:**
- F1 Score: 86-90% (current: 82.4%)
- Recall: 84-90% (current: 74%)
- Precision: >89% (current: 93%)
- Processing time: <10% increase

**Validation:**
- Test on 739-name dataset
- Run on held-out validation set
- Compare with baseline metrics
- Manual review of new false positives

---

## Questions?

See detailed reports:
- **Full Analysis:** `FALSE_NEGATIVE_ANALYSIS_REPORT.md`
- **Detailed Recommendations:** `IMPROVEMENT_RECOMMENDATIONS_SUMMARY.md`
- **Implementation Code:** `backend/app/services/name_matcher.py`

---

**TL;DR:**
1. Change one line (threshold from 98% to 90%) â†’ +8-12% recall
2. Add acronym detection â†’ +5-8% recall
3. Result: F1 86-90% (production ready)
